{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93408d5f-c32c-41da-b567-22f42fe76354",
   "metadata": {},
   "source": [
    "# Automation of Job Search in Guatemala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50656198-9127-485a-bdc3-c61d05261f34",
   "metadata": {},
   "source": [
    "### Developed by\n",
    "\n",
    "**Cristofer Darwin Berganza**\n",
    "\n",
    "Email 1: cdberganza@gmail.com\n",
    "\n",
    "Email 2: cdberganza@proton.me\n",
    "\n",
    "GitHub: [GitHub portfolio](https://github.com/cdberganza/portfolio)\n",
    "\n",
    "LinkedIn: [LinkedIn profile](https://www.linkedin.com/in/darwin-berganza/)\n",
    "\n",
    "If you have any questions or comments about this project, feel free to reach out to me.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8f013-2782-4773-a3cd-0a7a5a724601",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Project Description](#Project-Description)\n",
    "3. [Code](#Code)\n",
    "4. [Capture of the generated file](#Capture-of-the-generated-file)\n",
    "5. [Conclusions](#Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4369f36-568f-4a70-a52c-7e71afa200eb",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project aims to automate the search for job offers on job websites in Guatemala. The developed solution allows the company's collaborators to quickly obtain CSV files with information about available job vacancies, saving time and effort in manual searching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b9e9ae-35f3-4d9c-9879-e38486712b1c",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "The main objective of this project is to facilitate the search for job vacancies by automating the process of gathering information from job websites. Through this algorithm, users will be able to input the job title or category they wish to search for, and the system will automatically generate a CSV file containing all available offers.\n",
    "\n",
    "In the development of the code, an effort has been made to use function and variable names in English, following best programming practices. However, since the algorithm will be used by Spanish speakers in Guatemala, communication with the user through messages in `input()` and `print()`, as well as the headers in the CSV file, has been done in Spanish. This ensures that the tool is accessible and user-friendly for the target audience.\n",
    "\n",
    "The process is divided into several key stages:\n",
    "1. **URL Generation**: A dynamic URL is generated based on the job title or category entered by the user.\n",
    "2. **Data Extraction**: Using `BeautifulSoup`, relevant details from each job listing are extracted, such as job title, company, location, salary, and the link to the offer.\n",
    "3. **Saving to CSV**: The extracted data is saved in a CSV file, providing a clear and structured format that users can use to review the offers or share them.\n",
    "\n",
    "The code is structured into several functions that handle different aspects of the process, from URL generation to CSV file writing, making it easier to maintain and understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717464c8-2e5c-456b-94d0-98cce8ec856f",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "This Jupyter Notebook contains a set of functions that work together to extract job offers from a website and save them into a CSV file. As you progress through the code, you will see how each function has a specific purpose, and how they integrate to achieve the overall goal of the project.\n",
    "\n",
    "We invite you to read the code, experiment with it, and run it in your local environment. Feel free to make modifications to adapt it to your needs or to add new functionalities. Enjoy the learning process!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b218355-0294-43b1-a525-8e826c775e2a",
   "metadata": {},
   "source": [
    "### Installing Libraries\n",
    "\n",
    "This cell includes the commands needed to install the required libraries for the project. Make sure to run this cell before executing the rest of the code if the libraries are not already installed in your working environment. These libraries are essential for the program's functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24136dc-743c-45b9-b8bb-be45bb67ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests beautifulsoup4 numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf55999-49a7-439c-85ca-39ca2cf0db4a",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "In this cell, the necessary libraries for the project are imported:\n",
    "\n",
    "- `requests`: to make HTTP requests and access web pages.\n",
    "- `BeautifulSoup`: to parse the HTML content of web pages.\n",
    "- `numpy`: to handle numerical operations.\n",
    "- `csv`: to work with CSV files and save the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19df4b-9209-4125-9f92-98ed0f9ce04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab588cc9-198c-41d5-a930-9516a39f198f",
   "metadata": {},
   "source": [
    "### URL Generation\n",
    "\n",
    "In this cell, the function `gen_url()` is defined, allowing the user to enter the job title or category they wish to search for. The function performs the following tasks:\n",
    "\n",
    "1. Prompts the user to enter the job title or category via input.\n",
    "2. Replaces spaces in the user's input with hyphens to form part of the URL.\n",
    "3. Generates a specific URL to search for job offers on the Computrabajo website in Guatemala.\n",
    "4. Prints the generated URL for the user to see.\n",
    "5. Returns the generated URL.\n",
    "\n",
    "This function is essential for accessing job offers in an automated manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450500b-b075-4a16-8a18-9e1ba5ebf408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_url():\n",
    "    \n",
    "    job = input('Introduce el cargo o categoria: ')\n",
    "    job_mod = job.replace(' ', '-')\n",
    "    url = f'https://gt.computrabajo.com/trabajo-de-{job_mod}?p='\n",
    "    print(f'URL generada: {url}')\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971cb9d0-86df-46ec-bdb3-87216ec6d2ff",
   "metadata": {},
   "source": [
    "### Extraction of Job Offers\n",
    "\n",
    "In this cell, the function `extract_offers()` is defined, responsible for extracting job offers from the pages generated by the `gen_url()` function. The function performs the following tasks:\n",
    "\n",
    "1. Sets a `User-Agent` header to simulate a request from a web browser and avoid being blocked by the server.\n",
    "2. Initializes an empty list `offers` to store the found job offers.\n",
    "3. Uses a `for` loop to iterate over the first eight result pages (from 1 to 8).\n",
    "   - For each page, it makes an HTTP request using the generated URL and the established header.\n",
    "   - Prints the status code of the response for each page.\n",
    "   - Parses the content of the response using `BeautifulSoup`.\n",
    "   - Searches for all `article` elements with the class `box_offer`, which contain job offers.\n",
    "   - Adds the found offers to the `offers` list.\n",
    "4. Prints the total number of offers found.\n",
    "5. Returns the list of extracted offers.\n",
    "\n",
    "This function is crucial for gathering data on available job offers across several pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a74d39-0f5b-421d-b51c-f574974651cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_offers():\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    offers = []\n",
    "    for i in range(1, 9):\n",
    "        response = requests.get(url+str(i), headers=headers)\n",
    "        print(f\"Status code {str(i)}: {response.status_code}\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        offers_soup = soup.find_all('article', class_='box_offer')\n",
    "        \n",
    "        offers = offers + offers_soup\n",
    "        \n",
    "    print(f'Ofertas encontradas: {len(offers)}')\n",
    "    \n",
    "    return offers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097af3e3-9361-4867-b560-285fb840c1dd",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "\n",
    "In this cell, the function `extract_data(soup)` is defined, responsible for extracting specific information about each job offer from a BeautifulSoup `soup` object. The function performs the following tasks:\n",
    "\n",
    "1. Attempts to extract the job title:\n",
    "   - Searches for the link corresponding to the job title using the class `js-o-link fc_base`.\n",
    "   - If the title is found, it is stripped of whitespace. If not, `NaN` from NumPy is assigned.\n",
    "\n",
    "2. Attempts to extract the company name:\n",
    "   - Searches for the link corresponding to the company name using the class `fc_base t_ellipsis`.\n",
    "   - If the name is found, it is stripped of whitespace. If not, `NaN` is assigned.\n",
    "\n",
    "3. Attempts to extract the location:\n",
    "   - Searches for the paragraph corresponding to the location using the class `fs16 fc_base mt5`.\n",
    "   - If the location is found, it is stripped of whitespace. If not, `NaN` is assigned.\n",
    "\n",
    "4. Attempts to extract the salary:\n",
    "   - Searches for the div corresponding to the salary using the class `fs13 mt15`.\n",
    "   - If the salary is found, it is stripped of whitespace. If not, `NaN` is assigned.\n",
    "\n",
    "5. Generates the job offer URL by concatenating the link from the title with the base URL of Computrabajo.\n",
    "\n",
    "6. Returns a list containing the job title, company, location, salary, and offer URL.\n",
    "\n",
    "This function is essential for structuring and storing the relevant information of each found job offer while managing potential errors in data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9271f-bbac-46f5-acf4-4c293b183883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(soup):\n",
    "    \n",
    "    try:\n",
    "        job_title_soup = soup.find('a', class_='js-o-link fc_base')\n",
    "        job_title = job_title_soup.text.strip()\n",
    "    except:\n",
    "        job_title = np.nan\n",
    "    \n",
    "    try:\n",
    "        company_soup = soup.find('a', class_='fc_base t_ellipsis')\n",
    "        company = company_soup.text.strip()\n",
    "    except:\n",
    "        company = np.nan\n",
    "    \n",
    "    try:\n",
    "        location_soup = soup.find('p', class_='fs16 fc_base mt5')\n",
    "        location = location_soup.text.strip()\n",
    "    except:\n",
    "        location = np.nan\n",
    "\n",
    "    try:\n",
    "        salary_soup = soup.find('div', class_='fs13 mt15')\n",
    "        salary = salary_soup.text.strip()\n",
    "    except:\n",
    "        salary = np.nan\n",
    "    \n",
    "    job_url = 'https://gt.computrabajo.com/'+job_title_soup['href']\n",
    "    \n",
    "    return [job_title, company, location, salary, job_url]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc6e3d-ae8f-4f62-8945-022f858c30de",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "\n",
    "In this cell, the function `gen_data(offers)` is defined, responsible for generating a list of structured data from the extracted job offers. The function performs the following tasks:\n",
    "\n",
    "1. Initializes an empty list called `data`, which will store the information extracted from each job offer.\n",
    "\n",
    "2. Uses a `for` loop to iterate over the offers received as an argument:\n",
    "   - For each offer, it calls the `extract_data()` function to extract the relevant information.\n",
    "   - Appends the result to the `data` list.\n",
    "\n",
    "3. Returns the `data` list, which contains the structured information of all processed offers.\n",
    "\n",
    "This function is essential for compiling the extracted data in an organized manner, facilitating its later use in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01141082-30f0-41be-9e44-a2bd53de6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(offers):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i, offer in enumerate(offers):\n",
    "        \n",
    "        data.append(extract_data(offers[i]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea87c0f-2a59-4881-b2f2-5135e64fc781",
   "metadata": {},
   "source": [
    "### File Name Generation\n",
    "\n",
    "In this cell, the function `gen_file_name()` is defined, allowing the user to specify the name of the CSV file that will be generated. The function performs the following tasks:\n",
    "\n",
    "1. Prompts the user to enter the file name through input.\n",
    "2. Appends the `.csv` extension to the name provided by the user.\n",
    "3. Returns the full name of the CSV file.\n",
    "\n",
    "This function is useful for customizing the name of the output file, making it easier to identify and manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef51fb3-4ec8-4cc4-9e58-ad346d0cc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_file_name():\n",
    "    file_name = input('Ingrese el nombre del archivo CSV: ')\n",
    "    file_name = file_name+'.csv'\n",
    "    return file_name  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af4a75-078b-403a-95de-eb49ac990e53",
   "metadata": {},
   "source": [
    "### Writing CSV File\n",
    "\n",
    "In this cell, the function `write_csv(file_name, data)` is defined, responsible for writing the extracted data to a CSV file. The function performs the following tasks:\n",
    "\n",
    "1. Defines a list `columns` containing the names of the columns in the CSV file: `Puesto`, `Empresa`, `Ubicación`, `Salario`, and `Enlace`.\n",
    "\n",
    "2. Opens a CSV file in write mode (`'w'`) with UTF-8 encoding:\n",
    "   - If the file already exists, it will be overwritten.\n",
    "\n",
    "3. Creates a `writer_csv` object using the `csv` module, allowing writing to the CSV file.\n",
    "\n",
    "4. Writes the header row to the CSV file using `writer_csv.writerow(columns)`.\n",
    "\n",
    "5. Iterates over the received data and writes each offer to the CSV file using `writer_csv.writerow(offer)`.\n",
    "\n",
    "This function is fundamental for saving the extracted information in a structured manner in a CSV file located in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4900a-8f88-463d-85f6-50760e13c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(file_name, data):\n",
    "\n",
    "    columns = ['Puesto', 'Empresa', 'Ubicación', 'Salario', 'Enlace']\n",
    "    \n",
    "    with open(file_name, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        \n",
    "        writer_csv = csv.writer(csv_file)\n",
    "        \n",
    "        writer_csv.writerow(columns)\n",
    "        \n",
    "        for offer in data:\n",
    "            writer_csv.writerow(offer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc140e-9822-4f00-94a5-5eebdf7bdb81",
   "metadata": {},
   "source": [
    "### Executing Functions to Obtain Data\n",
    "\n",
    "In this cell, the following operations are performed:\n",
    "\n",
    "1. The function `gen_url()` is called to generate the search URL for job offers based on the category or job title entered by the user. The generated URL is stored in the variable `url`.\n",
    "\n",
    "2. The function `extract_offers()` is called to extract job offers from the pages of the generated URL. The extracted offers are stored in the variable `offers`.\n",
    "\n",
    "3. The function `gen_data(offers)` is called to process the extracted offers and generate a list of structured data. This list is stored in the variable `data`.\n",
    "\n",
    "4. A message is printed indicating that the list of data has been generated and that the user can proceed to create the CSV file.\n",
    "\n",
    "This cell is key to executing the main flow of the program, integrating all the previously defined functions to obtain and organize the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8b1ec-9ef6-466b-9822-cc2dd4a88761",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = gen_url()\n",
    "offers = extract_offers()\n",
    "data = gen_data(offers)\n",
    "print('Lista de datos generada, ya puedes generar el archivo CSV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bc8c6-b9fb-4a0f-8b22-f8548675af78",
   "metadata": {},
   "source": [
    "### Generation and Writing of the CSV File\n",
    "\n",
    "In this cell, the following operations are performed to generate and save the CSV file:\n",
    "\n",
    "1. The function `gen_file_name()` is called to prompt the user for the name of the CSV file. The generated name is stored in the variable `file_name`.\n",
    "\n",
    "2. The function `write_csv(file_name, data)` is called to write the list of structured data to the CSV file with the name specified by the user.\n",
    "\n",
    "3. A confirmation message is printed indicating that the CSV file has been successfully saved.\n",
    "\n",
    "This cell is essential to finalize the program process, allowing the user to obtain a CSV file with the collected job offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a510520-9953-4f58-89c9-a047dd06d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = gen_file_name()\n",
    "write_csv(file_name, data)\n",
    "print('Archivo CSV guardado con éxito')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601277c-8ad8-4815-b634-89651f6d9caf",
   "metadata": {},
   "source": [
    "## Capture of the generated file\n",
    "\n",
    "Attached below is a screenshot of the file generated with the search for job offers for \"asesor de ventas\" (sales advisor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc7e26-d73a-43f6-bed1-38c013f38b28",
   "metadata": {},
   "source": [
    "![Captura del archivo generado](https://raw.githubusercontent.com/cdberganza/job_scraping_gt/refs/heads/main/img/capture_3_asesor_de_ventas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81381e-1d5c-41c1-8ab4-26ecb370b3c9",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This project has proven to be an effective solution for automating the search for job offers on job websites in Guatemala. Through the implementation of various functions, we successfully extracted and organized relevant information efficiently.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- **Automation:** Automating the search for job offers saves time and effort compared to manual searching.\n",
    "- **Data Structuring:** Collecting data in a CSV file facilitates its analysis and further use.\n",
    "- **Accessibility:** Communication in Spanish and user-centered design make the tool accessible to the target audience.\n",
    "\n",
    "**Future Improvements:**\n",
    "\n",
    "- **Graphical Interface:** Consider developing a graphical interface to further facilitate the use of the program.\n",
    "- **Error Handling:** Implement more robust error handling to address potential changes in the source websites.\n",
    "- **Expansion of Sources:** Explore the possibility of including more data sources to offer a wider range of job offers.\n",
    "\n",
    "This project has not only been a great opportunity to apply programming and web scraping skills but can also serve as a foundation for future developments in the job search domain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e79f97-7a87-4560-8aa8-bc05f4f7ed5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
